{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B_jvyUqMWc_P",
        "outputId": "5fd18d15-a04a-4624-9857-f64b8e0087bd"
      },
      "outputs": [],
      "source": [
        "# RUN it if you are on COLAB\n",
        "#!pip install datasets\n",
        "#!pip install SPARQLWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9HMX2-NXvqo"
      },
      "outputs": [],
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time\n",
        "from datasets import load_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4aVwVrZl16I",
        "outputId": "3435f1a8-6ae9-4329-b3ee-1b91e35872d6"
      },
      "outputs": [],
      "source": [
        "# Mount drive if you are on google colab\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjWBVm2MYe5Z"
      },
      "source": [
        "### Exploiting the wikidata knowledge-graph based structure\n",
        "\n",
        "To enrich the dataset we identified some relevant properties for the 19 categories that we are dealing with. The idea is to exploit the knowledge-graph based wikidata structure and to extract such relevant information using **SPARQL** as query language.\n",
        "\n",
        "We identified a total of 29 relevant properites:\n",
        "- **P495**: country of origin\n",
        "- **P2596**: culture\n",
        "- **P172**: ethnic group\n",
        "- **P37**: official language\n",
        "- **P407**: langauge of work or name\n",
        "- **P135**: Movement(Art, Literature, philosophy)\n",
        "- **P136**: Genre\n",
        "- **P921**: Main subject\n",
        "- **P547**: Memorialized by\n",
        "- **P784**: Significant event\n",
        "- **P840**: Narrative location\n",
        "- **P17**: Country\n",
        "- **P1843**: Taxon common name\n",
        "- **P1001**: Applies to jurisdiction\n",
        "- **P144**: Based on\n",
        "- **P361**: Part of\n",
        "- **P1705**: Native label\n",
        "- **P2012**: cuisine\n",
        "- **P2541**: Operating area\n",
        "- **P1535**: Used by\n",
        "- **P366**: Use\n",
        "- **P1142**: Political ideology\n",
        "- **P140**: Religion\n",
        "- **P102**: Member of political party\n",
        "- **P1344**: Participant in\n",
        "- **P183**: Endemic to\n",
        "- **P2341**: Indigenous to\n",
        "- **P1532**: Country for sport\n",
        "- **P279**: Subclass of\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eK9PBmCJaSqz"
      },
      "outputs": [],
      "source": [
        "properties = ['originLabel', 'cultureLabel', 'ethnic_groupLabel', 'off_languageLabel',\n",
        "                'nameLabel', 'movementLabel', 'genreLabel', 'main_subjectLabel',\n",
        "                'memorializedLabel', 'sign_eventLabel', 'narrative_locLabel', 'countryLabel',\n",
        "                'taxonLabel', 'jurisdictionLabel', 'based_onLabel', 'part_ofLabel',\n",
        "                'native_labelLabel', 'cuisineLabel', 'areaLabel', 'used_byLabel',\n",
        "                'useLabel', 'political_ideoLabel', 'religionLabel', 'political_partyLabel',\n",
        "                'participant_inLabel', 'endemicLabel', 'indigenousLabel',\n",
        "                'country_sportLabel', 'subclass_ofLabel'\n",
        "              ]\n",
        "\n",
        "# NOTE: we add Lable at the end of the name because we are interested in the lable, and not\n",
        "# the wikidata identifier (e.g. Q177, identifier for pizza)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bg6s5G3ibUqJ"
      },
      "outputs": [],
      "source": [
        "# To interact with wikidata we are going to use the SPARQLWrapper library\n",
        "sparql = SPARQLWrapper('https://query.wikidata.org/sparql', agent='GjWikidataBot/1.0')\n",
        "sparql.setReturnFormat(JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2d_ojsF-b5NM"
      },
      "outputs": [],
      "source": [
        "# To extract information from wikidata we are going to need only the wikidata\n",
        "# identifier. So now we define two different functions to extract them\n",
        "\n",
        "def extract_test_IDs(dataset):\n",
        "    test_IDs = []\n",
        "    for item in dataset['item']:\n",
        "        item_id = item.strip().split('/')[-1]\n",
        "        test_IDs.append(item_id)\n",
        "\n",
        "    return test_IDs\n",
        "\n",
        "def extract_IDs(dataset):\n",
        "    silver_IDs = []\n",
        "    gold_IDs = []\n",
        "\n",
        "    silver_samples = len(dataset['train'])\n",
        "    for index in range(silver_samples):\n",
        "        item_id = dataset['train'][index]['item'].strip().split('/')[-1]\n",
        "        silver_IDs.append(item_id)\n",
        "\n",
        "    gold_samples = len(dataset['validation'])\n",
        "    for index in range(gold_samples):\n",
        "        item_id = dataset['validation'][index]['item'].strip().split('/')[-1]\n",
        "        gold_IDs.append(item_id)\n",
        "\n",
        "    return silver_IDs, gold_IDs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TmwpujPHnLnH"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "datasets = load_from_disk('../datasets/train_and_val')\n",
        "test_set = pd.read_csv('../datasets/test/testset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sCMIWR-Ydu7j"
      },
      "outputs": [],
      "source": [
        "silver_IDs, gold_IDs = extract_IDs(datasets)\n",
        "test_IDs = extract_test_IDs(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IwJomn32g5Qr"
      },
      "outputs": [],
      "source": [
        "# Auxiliary function used to handle the response from the server\n",
        "def get_query_result(data, properties):\n",
        "    query_result = {}\n",
        "    keys = data['results']['bindings'][0].keys()\n",
        "    for prop in properties:\n",
        "        if prop in keys:\n",
        "            query_result[prop] = data['results']['bindings'][0][prop]['value']\n",
        "        else:\n",
        "            query_result[prop] = None\n",
        "\n",
        "    return query_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQz7VMyEeO2I"
      },
      "source": [
        "In the following we are going to define the function that extracts the data from wikidata. The main part of the function is the `SPARQL` query. In the `SELECT` clause we insert the variables to which we are intersted. In the `WHERE` clause we insert conditions of the following form:\n",
        "`OPTIONAL wd:item_id wdt:PROP_id ?var .`\n",
        "\n",
        "where:\n",
        "- OPTIONAL is inserted to keep the result also in case of missing values (by default, if a variable is NULL, the returned result is an empty string)\n",
        "- item_id: is the identifier of the item\n",
        "- PROP_id is the idenfier of the propertie (e.g. country of origin has identifier **P495**)\n",
        "- ?var: is the value we want to retrieve (`?` indicates that var is a variable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t1j05lkFgjP8"
      },
      "outputs": [],
      "source": [
        "def retrieve_info(items):\n",
        "    i = 1                # to handle sleeping time\n",
        "    dict_result = {}     # dictionary storing query results\n",
        "    num_items = len(items)\n",
        "    k = 0\n",
        "    while k < num_items:\n",
        "        item = items[k]\n",
        "        sparql.setQuery(\n",
        "            f'''\n",
        "            SELECT ?originLabel ?cultureLabel ?ethnic_groupLabel ?off_languageLabel\n",
        "                    ?nameLabel ?movementLabel ?genreLabel ?main_subjectLabel\n",
        "                    ?memorializedLabel ?sign_eventLabel ?narrative_locLabel ?countryLabel\n",
        "                    ?taxonLabel ?jurisdictionLabel ?based_onLabel ?part_ofLabel\n",
        "                    ?native_labelLabel ?cuisineLabel ?areaLabel ?used_byLabel\n",
        "                    ?useLabel ?political_ideoLabel ?religionLabel ?political_partyLabel\n",
        "                    ?participant_inLabel ?endemicLabel ?indigenousLabel\n",
        "                    ?country_sportLabel ?subclass_ofLabel\n",
        "            WHERE {{\n",
        "                OPTIONAL {{ wd:{item} wdt:P495 ?origin .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P2596 ?culture .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P172 ?ethnic_group .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P37 ?off_language .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P407 ?name .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P135 ?movement .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P136 ?genre .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P921 ?main_subject .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P547 ?memorialized .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P793 ?sign_event .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P840 ?narrative_loc .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P17 ?country .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1843 ?taxon .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1001 ?jurisdiction .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P144 ?based_on .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P361 ?part_of .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1705 ?native_label .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P2012 ?cuisine .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P2541 ?area .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1535 ?used_by .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P366 ?use .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1142 ?political_ideo .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P140 ?religion .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P102 ?political_party .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1344 ?participant_in .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P183 ?endemic .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P2341 ?indigenous .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P1532 ?country_sport .}}\n",
        "                OPTIONAL {{ wd:{item} wdt:P279 ?subclass_of .}}\n",
        "\n",
        "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],mul,en\". }}\n",
        "\n",
        "            }}\n",
        "\n",
        "            LIMIT 1\n",
        "        '''\n",
        "        )\n",
        "\n",
        "        data = ''\n",
        "        try:\n",
        "            data = sparql.queryAndConvert()\n",
        "            query_result = get_query_result(data, properties)\n",
        "            dict_result[item] = query_result\n",
        "            k += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # We increment the sleeping time when wikidata\n",
        "            # complains because we are sending too many requests\n",
        "            time.sleep(10 * i)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "    return dict_result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BGqVxH4hT5e"
      },
      "source": [
        "Now we can retrieve the information from wikidata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PqI0WUVchaQz"
      },
      "outputs": [],
      "source": [
        "# retrieve info for the silver dataset\n",
        "dict_training = retrieve_info(silver_IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8e7bq1_kiHaB"
      },
      "outputs": [],
      "source": [
        "# retrieve info for the dev dataset\n",
        "dict_validation = retrieve_info(gold_IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "88bMWZPRiIlW"
      },
      "outputs": [],
      "source": [
        "# retrieve info for the test dataset\n",
        "dict_test = retrieve_info(test_IDs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6PWSD79kKFK"
      },
      "source": [
        "We convert the dictionaries to pandas DataFrame and then save everything as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BqSECOQkiiqH"
      },
      "outputs": [],
      "source": [
        "training_table = pd.DataFrame.from_dict(dict_training, orient='index')\n",
        "training_table = training_table.reset_index()\n",
        "training_table.rename(columns={'index': 'item'}, inplace=True)\n",
        "training_table.to_csv('../datasets/properties/training_props.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UNfXmRCtjiNf"
      },
      "outputs": [],
      "source": [
        "validation_table = pd.DataFrame.from_dict(dict_validation, orient='index')\n",
        "validation_table = validation_table.reset_index()\n",
        "validation_table.rename(columns={'index': 'item'}, inplace=True)\n",
        "validation_table.to_csv('../datasets/properties/validation_props.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "g8XI3zS7kHn0"
      },
      "outputs": [],
      "source": [
        "test_table = pd.DataFrame.from_dict(dict_test, orient='index')\n",
        "test_table = test_table.reset_index()\n",
        "test_table.rename(columns={'index': 'item'}, inplace=True)\n",
        "test_table.to_csv('../datasets/properties/test_props.csv', index=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
