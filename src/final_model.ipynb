{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edcd904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 11:07:15.644314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d7631",
   "metadata": {},
   "source": [
    "In this notebook we are going to integrate three different models that we trained on three different 'kind' of data that capture different aspects of each item. More precisely we have trained three different models:\n",
    "- `RoBERTa`: for analyzing text (the description of the item).\n",
    "- Two `MLP`, one for analyzing the the properties and one for analyzing statistical information and categorical features.\n",
    "\n",
    "The idea here is to integrate all these three models, putting on top of them a `MLP` that performes the classification task.\n",
    "The final model is the following:\n",
    "\n",
    "<img src=\"../images/final_model.png\" alt=\"Final model\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8461cf",
   "metadata": {},
   "source": [
    "In the following we present the architecture for the two `MLP`. Actually, here we are intersted only in extracting the features learned by the two models, so we modified a bit the architecture to extract such fetaures from the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8c5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesClassifier(nn.Module):\n",
    "    '''\n",
    "    input_dim = 29 (the number of properties that we used)\n",
    "    hidden1 = 20\n",
    "    hidden2 = 10\n",
    "    out_dim = 3 (the number of classes)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden1, hidden2, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_dim)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        embedding = self.act_fn(x)\n",
    "        x = self.fc3(embedding)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x, embedding     # ‚Üê we are intersted in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4bb8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsClassifier(nn.Module):\n",
    "    '''\n",
    "    input_dim = 144 (number of attributes resulting after the one-hot encoding)\n",
    "    hidden1 = 70\n",
    "    hidden2 = 10\n",
    "    out_dim = 3 (the number of classes)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden1, hidden2, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_dim)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        embedding = self.act_fn(x)\n",
    "        x = self.fc3(embedding)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979dca",
   "metadata": {},
   "source": [
    "Now we proceed with loading the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1ac07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RoBERTa\n",
    "roberta = AutoModelForSequenceClassification.from_pretrained(\"../models/RoBERTa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/RoBERTa\")\n",
    "\n",
    "# Properties classifier\n",
    "prop_state_dict = torch.load(\"../models/prop_classifier.tar\", weights_only=True)\n",
    "prop_classifier = PropertiesClassifier(input_dim=29, hidden1=20, hidden2=10, out_dim=3)\n",
    "prop_classifier.load_state_dict(prop_state_dict)\n",
    "\n",
    "# Statistical classifier\n",
    "stats_state_dict = torch.load(\"../models/stats_classifier.tar\", weights_only=True)\n",
    "stats_classifier = StatsClassifier(input_dim=144, hidden1=70, hidden2=10, out_dim=3)\n",
    "stats_classifier.load_state_dict(stats_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034bfd5",
   "metadata": {},
   "source": [
    "In the following we define a functions for extracting the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2859cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting text emebdding from RoBERTa\n",
    "def get_embeddings_from_roberta(text, model, tokenizer, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", max_length=max_length,\n",
    "                         padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # extract the hidden states\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        # Here we consider only the last layer\n",
    "        embeddings = hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n",
    "        embeddings = embeddings.squeeze(dim=0) # shape: (sequence_length, hidden_size)\n",
    "\n",
    "        # Since some of the tokens can be padding tokens, we don't want\n",
    "        # to consider them, so we perform an element-wise multiplication\n",
    "        # to set them to zero.\n",
    "        attention_mask = attention_mask.float()\n",
    "        attention_mask = torch.transpose(attention_mask, 0, 1)\n",
    "\n",
    "        sentence_embedding = embeddings * attention_mask\n",
    "\n",
    "        # Note, we don't use torch.mean but instead we sum over the rows and then\n",
    "        # divide by the numer of non-padding tokens (attention_mask.sum() gives us\n",
    "        # exactly this infomration). This way we take somehow into account the\n",
    "        # length of the sequence\n",
    "        sentence_embedding = torch.sum(sentence_embedding, dim=0) / attention_mask.sum()\n",
    "\n",
    "    return sentence_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0d2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the embeddings from the MLP\n",
    "def get_embeddings_from_MLP(model, input):\n",
    "  model.eval()\n",
    "  _, embedding = model.forward(input)\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a43ab",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "Now, we build the `MLP` to put on top of the three models that we trained before and perform the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128071a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalMLP(nn.Module):\n",
    "    '''\n",
    "    1. sentence embedding, size: 768\n",
    "    2. properties features, size: 10\n",
    "    3. statistical features, size: 10\n",
    "\n",
    "    input_dim = 788\n",
    "    hidden = 500\n",
    "    out_dim = 3\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden1, hidden2, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_dim)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984d9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the model\n",
    "final_model = FinalMLP(input_dim=788, hidden1=500, hidden2=250, out_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce760021",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Now we are going to prepare the dataset to train the final model. What we are going to do is to extract the embeddings for the different kinds of data (text, properties, statistics and categorical) and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d4ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'cultural agnostic': 0.0, 'cultural representative': 1.0, 'cultural exclusive': 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfb9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedDataset(data.Dataset):\n",
    "\n",
    "    def get_sentence_embedding(self, text):\n",
    "        # extract the sentence embedding from roberta\n",
    "        sentence_embedding = get_embeddings_from_roberta(text, roberta, tokenizer)\n",
    "\n",
    "        return sentence_embedding\n",
    "\n",
    "    def get_prop_features(self, row):\n",
    "        '''\n",
    "        Here we prepare the data for the properties. We going to use a kind\n",
    "        of one-hot encoding; for each item, we create a vector that has 1.0,\n",
    "        if in the corresponing position the property is not None, 0.0 otherwise.\n",
    "        E.g.:\n",
    "        item = [Italy, None, None, Sicily, None, ...]\n",
    "        vector = [1.0, 0.0, 0.0, 1.0, 0.0, ...]\n",
    "        '''\n",
    "\n",
    "        vector = []\n",
    "        for value in row:\n",
    "            vector.append(1.0 if not pd.isna(value) else 0.0)\n",
    "\n",
    "        return vector\n",
    "\n",
    "    def __init__(self, desc_dataset, prop_dataset, stats_dataset, labels):\n",
    "        '''\n",
    "        desc_dataset: dataset containing item's descriptions\n",
    "        prop_dataset: dataset containing the values of item's properties\n",
    "        stats_dataset: dataset containing the statistical information\n",
    "        '''\n",
    "\n",
    "        self.num_samples = len(desc_dataset)\n",
    "\n",
    "        self.data = []\n",
    "        for i in range(self.num_samples):\n",
    "\n",
    "            # we set torch.no_grad() because here we don't wan't to keep track of the gradients\n",
    "            with torch.no_grad():\n",
    "\n",
    "                text_embedding = self.get_sentence_embedding(desc_dataset['description'][i]).detach()\n",
    "\n",
    "                prop_tensor = torch.tensor(self.get_prop_features(prop_dataset.iloc[i]), dtype=torch.float32)\n",
    "                prop_features = get_embeddings_from_MLP(prop_classifier, prop_tensor).detach()\n",
    "\n",
    "                stats_tensor = torch.tensor(stats_dataset[i], dtype=torch.float32)\n",
    "                stats_features = get_embeddings_from_MLP(stats_classifier, stats_tensor).detach()\n",
    "\n",
    "                # we concatenate the different embeddings\n",
    "                integrated_input = torch.cat((text_embedding, prop_features, stats_features), 0)\n",
    "\n",
    "                if labels is not None:\n",
    "                  label = torch.tensor(mapping[labels.iloc[i]])\n",
    "\n",
    "                # the final dataset will be a list of dictionaries with two keys:\n",
    "                # inputs: concatenated embeddings\n",
    "                # outputs: label\n",
    "                # or simply inputs for the test set\n",
    "                if labels is not None:\n",
    "                  sample = {'inputs': integrated_input, 'outputs': label}\n",
    "                else:\n",
    "                  sample = {'inputs': integrated_input}\n",
    "\n",
    "                self.data.append(sample)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ef05b",
   "metadata": {},
   "source": [
    "Since there are some mismatches between the various datasets (properties, statistcs and decription), we do some cleaning.  \n",
    "NOTE: the mismatch is due to the fact that we had to remove some rows from the statistical datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4f21bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_props = pd.read_csv('../datasets/properties/training_props.csv')\n",
    "val_props = pd.read_csv('../datasets/properties/validation_props.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f12d1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stats datasets\n",
    "train_stats = pd.read_csv('../datasets/statistics/train_stats.csv')\n",
    "val_stats = pd.read_csv('../datasets/statistics/val_stats.csv')\n",
    "\n",
    "# In the item column we still have the full links (http://www.wikidata.org/entity/Q32786)\n",
    "# therefore we modify it to keep only the item's identifier (Q32786)\n",
    "train_stats['item'] = train_stats['item'].apply(lambda x: x.strip().split('/')[-1])\n",
    "val_stats['item'] = val_stats['item'].apply(lambda x: x.strip().split('/')[-1])\n",
    "\n",
    "# Finally we remove from the properties datasets the items that are not present in the\n",
    "# statistical datasets\n",
    "cleaned_train_props = train_props[train_props['item'].isin(train_stats['item'])]\n",
    "cleaned_val_props = val_props[val_props['item'].isin(val_stats['item'])]\n",
    "\n",
    "# We reset the index\n",
    "cleaned_train_props = cleaned_train_props.reset_index(drop=True)\n",
    "cleaned_val_props = cleaned_val_props.reset_index(drop=True)\n",
    "\n",
    "# Finally, we drop the first two columns (index, item) since we don't need them\n",
    "cleaned_train_props = cleaned_train_props.drop(columns=['Unnamed: 0','item'])\n",
    "cleaned_val_props = cleaned_val_props.drop(columns=['Unnamed: 0','item'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03665cdb",
   "metadata": {},
   "source": [
    "Now we need to do the same for the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3224ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk('../datasets/train_and_val')\n",
    "train_descirptions = pd.DataFrame({'item': list(datasets['train']['item']), 'description': list(datasets['train']['description'])})\n",
    "val_descriptions = pd.DataFrame({'item': list(datasets['validation']['item']), 'description': list(datasets['validation']['description'])})\n",
    "\n",
    "# As before, we keep only the item's idetifier\n",
    "train_descirptions['item'] = train_descirptions['item'].apply(lambda x: x.strip().split('/')[-1])\n",
    "val_descriptions['item'] = val_descriptions['item'].apply(lambda x: x.strip().split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49d3d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_descriptions = train_descirptions[train_descirptions['item'].isin(train_stats['item'])]\n",
    "cleaned_val_descriptions = val_descriptions[val_descriptions['item'].isin(val_stats['item'])]\n",
    "\n",
    "cleaned_train_descriptions = cleaned_train_descriptions.reset_index(drop=True)\n",
    "cleaned_val_descriptions = cleaned_val_descriptions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b8beb",
   "metadata": {},
   "source": [
    "In the stats datasets we retrieved some useful statistical data, that we thought could help to perform the classification task. Precisey we stored the following informations:\n",
    "\n",
    "We have columns that store information about the text distribution, and more precisely we have the following attributes:\n",
    "- text_distribution: array storing length if wikipedia texts connected to the item\n",
    "- avg_text: mean of the text distribution quantity\n",
    "- std_text: standard deviation of the distrinution quantity\n",
    "- len: number of wikipedia links connected to the item\n",
    "- entropy_text: entropy computed on the text_distrinution\n",
    "- gini_text: gini's index computed on text_distribution\n",
    "- sum_over_texts: sum of the values in text distribution\n",
    "\n",
    "\n",
    "Then we have another set of columns storing data about the references to each item's wikipedia page. So similarly to before we have:\n",
    "- ref_distribution: array storing the number of references to each item wikipedia page\n",
    "- avg_ref: mean of the ref_distribution quantity\n",
    "- std_ref: standard deviation of the ref_distribution quantity\n",
    "- entropy_ref: entropy computed on the ref_distrbution\n",
    "- gini_ref: gini's index computed on the ref_distribution\n",
    "- sum_over_ref: sum of the values in the ref_didtribution quantity\n",
    "\n",
    "Finally, we have otther four columns:\n",
    "- h_adj_descr: euristic counting the number of attributes (italian, chinese, english,...) on the description\n",
    "- h_nat_descr: euristic counting the number of nations in the description of the item\n",
    "- h_adj_engtext: euristic counting the number of attributes (italian, chinese, english,...) on the english wikipedia text\n",
    "- h_nat_engtext: euristic counting the number of nations in the english wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38a0ee",
   "metadata": {},
   "source": [
    "Here we are going to keep also some categorical features that are useful in performing the classification task. To deal with this we are going to perform a **one-hot** encoding on these attributes. Moreover, it is useful to normalize the statistical features, since they have different scales. To do so we are going to use the `StandardScaler` module from `sklearn`, that applies the following transformation to the data:\n",
    "$$\\tilde{x} = \\frac{x - \\mu}{\\sigma}$$\n",
    "where $\\mu$ is the mean of the trainig samples and $\\sigma$ the standard deviation. With this transformation the whole dataset will have, approximately. mean 0 and standard deviation 1.\n",
    "\n",
    "Since we have two different tranformations to apply to the dataset we are going to use the `ColumnTranformer` module from sklearn, that simplifies considerably the whole procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cebb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "categorical_features = ['type', 'category', 'subcategory']\n",
    "stats_features = ['std_text', 'avg_text', 'len', 'entropy_text', 'gini_text',\n",
    "       'sum_over_texts', 'std_ref', 'avg_ref', 'sum_over_ref', 'entropy_ref',\n",
    "       'gini_ref', 'h_adj_descr', 'h_nat_descr', 'h_adj_engtext',\n",
    "       'h_nat_engtext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b65a9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to drop the text_distribution and ref_distribution columns.\n",
    "# We do so because they have a different type of data structure (array) with respect\n",
    "# to the other attributes (scalars) and so it is a bit more complicated to deal with\n",
    "# both kind of data. Moreover, there are other attributes that store all the relevant information\n",
    "# about these two distributions. Then we also drop the 'item', 'name' and 'label' column\n",
    "\n",
    "train_labels = train_stats['label']\n",
    "train_stats = train_stats.drop(columns=['item', 'name', 'label',\n",
    "                                          'countryLabel', 'subclass_ofLabel', 'text_distribution',\n",
    "                                          'ref_distribution'])\n",
    "val_labels = val_stats['label']\n",
    "val_stats = val_stats.drop(columns=['item', 'name', 'label',\n",
    "                                          'countryLabel', 'subclass_ofLabel', 'text_distribution',\n",
    "                                          'ref_distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c628c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [(\"text_preprocess\", encoder, categorical_features),\n",
    "     ('stats_preprocess', StandardScaler(), stats_features)]\n",
    ")\n",
    "\n",
    "## tranformerd datasets\n",
    "transformed_train_stats = ct.fit_transform(train_stats)\n",
    "transformed_val_stats = ct.transform(val_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7773757",
   "metadata": {},
   "source": [
    "Before proceeding, let's male a fast recap of the datasets that we have:\n",
    "\n",
    "- Two datasets containing the values of the properties:\n",
    "    - cleaned_train_props\n",
    "    - cleaned_val_props\n",
    "- Two datasets containing the descriptions:\n",
    "    - cleaned_train_descriptions\n",
    "    - cleaned_val_descriptions\n",
    "- Two datasets containing statistical information:\n",
    "    - transformed_train_stats\n",
    "    - tranformed_val_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9799e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6193\n",
      "6193\n",
      "6193\n",
      "6193\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_train_descriptions))\n",
    "print(len(cleaned_train_props))\n",
    "print(len(transformed_train_stats))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8baf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert len(cleaned_train_descriptions) == len(cleaned_train_props) == len(transformed_train_stats) == len(train_labels)\n",
    "\n",
    "assert len(cleaned_val_descriptions) == len(cleaned_val_props) == len(transformed_val_stats) == len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate the training and validation datasets\n",
    "\n",
    "training_samples = IntegratedDataset(desc_dataset=cleaned_train_descriptions,\n",
    "                                     prop_dataset=cleaned_train_props,\n",
    "                                     stats_dataset=transformed_train_stats,\n",
    "                                     labels=train_labels)\n",
    "\n",
    "validation_samples = IntegratedDataset(desc_dataset=cleaned_val_descriptions,\n",
    "                                      prop_dataset=cleaned_val_props,\n",
    "                                      stats_dataset=transformed_val_stats,\n",
    "                                      labels=val_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
