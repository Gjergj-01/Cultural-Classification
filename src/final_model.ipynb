{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edcd904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 11:29:36.773491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d7631",
   "metadata": {},
   "source": [
    "In this notebook we are going to integrate three different models that we trained on three different 'kind' of data that capture different aspects of each item. More precisely we have three trained different models:\n",
    "- `RoBERTa`: for analyzing text (the description of the item).\n",
    "- Two `MLP`, one for analyzing the the properties and one for analyzing statistical information and categorical features.\n",
    "\n",
    "The idea here is to integrate all these three models, putting on top of them a `MLP` that performes the classification task.\n",
    "The final model is the following:\n",
    "\n",
    "<img src=\"../images/final_model.png\" alt=\"Final model\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8461cf",
   "metadata": {},
   "source": [
    "In the following we present the architecture for the two `MLP`. Actually, here we are intersted only in extracting the features learned by the two models, so we modified a bit the architecture to extract such fetaures from the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertiesClassifier(nn.Module):\n",
    "    '''\n",
    "    input_dim = 29 (the number of properties that we used)\n",
    "    hidden1 = 20\n",
    "    hidden2 = 10\n",
    "    out_dim = 3 (the number of classes)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden1, hidden2, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_dim)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        embedding = self.act_fn(x)\n",
    "        x = self.fc3(embedding)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x, embedding     # ‚Üê we are intersted in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bb8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsClassifier(nn.Module):\n",
    "    '''\n",
    "    input_dim = 144 (number of attributes resulting after the one-hot encoding)\n",
    "    hidden1 = 70\n",
    "    hidden2 = 10\n",
    "    out_dim = 3 (the number of classes)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden1, hidden2, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_dim)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        embedding = self.act_fn(x)\n",
    "        x = self.fc3(embedding)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979dca",
   "metadata": {},
   "source": [
    "Now we proceed with loading the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ac07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa\n",
    "roberta = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/AILovePython_Shared_Folder/models/RoBERTa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/AILovePython_Shared_Folder/models/RoBERTa\")\n",
    "\n",
    "# Properties classifier\n",
    "prop_state_dict = torch.load(\"/content/drive/MyDrive/AILovePython_Shared_Folder/models/prop_classifier.tar\", weights_only=True)\n",
    "prop_classifier = PropertiesClassifier(input_dim=29, hidden1=20, hidden2=10, out_dim=3)\n",
    "prop_classifier.load_state_dict(prop_state_dict)\n",
    "\n",
    "# Statistical classifier\n",
    "stats_state_dict = torch.load(\"/content/drive/MyDrive/AILovePython_Shared_Folder/models/stats_classifier.tar\", weights_only=True)\n",
    "stats_classifier = StatsClassifier(input_dim=144, hidden1=70, hidden2=10, out_dim=3)\n",
    "stats_classifier.load_state_dict(stats_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034bfd5",
   "metadata": {},
   "source": [
    "In the following we define a functions for extracting the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2859cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting text emebdding from RoBERTa\n",
    "def get_embeddings_from_roberta(text, model, tokenizer, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", max_length=max_length,\n",
    "                         padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # extract the hidden states\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        # Here we consider only the last layer\n",
    "        embeddings = hidden_states[-1]  # (batch_size, sequence_length, hidden_size)\n",
    "        embeddings = embeddings.squeeze(dim=0) # shape: (sequence_length, hidden_size)\n",
    "\n",
    "        # Since some of the tokens can be padding tokens, we don't want\n",
    "        # to consider them, so we perform an element-wise multiplication\n",
    "        # to set them to zero.\n",
    "        attention_mask = attention_mask.float()\n",
    "        attention_mask = torch.transpose(attention_mask, 0, 1)\n",
    "\n",
    "        sentence_embedding = embeddings * attention_mask\n",
    "\n",
    "        # Note, we don't use torch.mean but instead we sum over the rows and then\n",
    "        # divide by the numer of non-padding tokens (attention_mask.sum() gives us\n",
    "        # exactly this infomration). This way we take somehow into account the\n",
    "        # length of the sequence\n",
    "        sentence_embedding = torch.sum(sentence_embedding, dim=0) / attention_mask.sum()\n",
    "\n",
    "    return sentence_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0d2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the embeddings from the MLP\n",
    "def get_embeddings_from_MLP(model, input):\n",
    "  model.eval()\n",
    "  _, embedding = model.forward(input)\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a43ab",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "Now, we build the `MLP` to put on top of the three models that we trained before and perform the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128071a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalMLP(nn.Module):\n",
    "    '''\n",
    "    1. sentence embedding, size: 768\n",
    "    2. properties features, size: 10\n",
    "    3. statistical features, size: 10\n",
    "\n",
    "    input_dim = 788\n",
    "    hidden = 500\n",
    "    out_dim = 3\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden1, hidden2, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_dim)\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the model\n",
    "final_model = FinalMLP(input_dim=788, hidden1=500, hidden2=250, out_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce760021",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Now we are going to prepare the dataset to train the final model. What we are going to do is to extract the embeddings for the different kinds of data (text, properties, statistics and categorical) and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'cultural agnostic': 0.0, 'cultural representative': 1.0, 'cultural exclusive': 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedDataset(data.Dataset):\n",
    "\n",
    "    def get_sentence_embedding(self, text):\n",
    "        # extract the sentence embedding from roberta\n",
    "        sentence_embedding = get_embeddings_from_roberta(text, roberta, tokenizer)\n",
    "\n",
    "        return sentence_embedding\n",
    "\n",
    "    def get_prop_features(self, row):\n",
    "        '''\n",
    "        Here we prepare the data for the properties. We going to use a kind\n",
    "        of one-hot encoding; for each item, we create a vector that has 1.0,\n",
    "        if in the corresponing position the property is not None, 0.0 otherwise.\n",
    "        E.g.:\n",
    "        item = [Italy, None, None, Sicily, None, ...]\n",
    "        vector = [1.0, 0.0, 0.0, 1.0, 0.0, ...]\n",
    "        '''\n",
    "\n",
    "        vector = []\n",
    "        for value in row:\n",
    "            vector.append(1.0 if not pd.isna(value) else 0.0)\n",
    "\n",
    "        return vector\n",
    "\n",
    "    def __init__(self, desc_dataset, prop_dataset, stats_dataset, labels):\n",
    "        '''\n",
    "        desc_dataset: dataset containing item's descriptions\n",
    "        prop_dataset: dataset containing the values of item's properties\n",
    "        stats_dataset: dataset containing the statistical information\n",
    "        '''\n",
    "\n",
    "        self.num_samples = len(desc_dataset)\n",
    "\n",
    "        self.data = []\n",
    "        for i in range(self.num_samples):\n",
    "\n",
    "            # we set torch.no_grad() because here we don't wan't to keep track of the gradients\n",
    "            with torch.no_grad():\n",
    "\n",
    "                text_embedding = self.get_sentence_embedding(desc_dataset['description'][i]).detach()\n",
    "\n",
    "                prop_tensor = torch.tensor(self.get_prop_features(prop_dataset.iloc[i]), dtype=torch.float32)\n",
    "                prop_features = get_embeddings_from_MLP(prop_classifier, prop_tensor).detach()\n",
    "\n",
    "                stats_tensor = torch.tensor(stats_dataset[i], dtype=torch.float32)\n",
    "                stats_features = get_embeddings_from_MLP(stats_classifier, stats_tensor).detach()\n",
    "\n",
    "                # we concatenate the different embeddings\n",
    "                integrated_input = torch.cat((text_embedding, prop_features, stats_features), 0)\n",
    "\n",
    "                if labels is not None:\n",
    "                  label = torch.tensor(mapping[labels.iloc[i]])\n",
    "\n",
    "                # the final dataset will be a list of dictionaries with two keys:\n",
    "                # inputs: concatenated embeddings\n",
    "                # outputs: label\n",
    "                # or simply inputs for the test set\n",
    "                if labels is not None:\n",
    "                  sample = {'inputs': integrated_input, 'outputs': label}\n",
    "                else:\n",
    "                  sample = {'inputs': integrated_input}\n",
    "\n",
    "                self.data.append(sample)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ef05b",
   "metadata": {},
   "source": [
    "Since there are some mismatches between the various datasets (properties, statistcs and decription), we do some cleaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
