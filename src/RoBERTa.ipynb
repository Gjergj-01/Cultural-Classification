{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7e97d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 20:52:13.284731: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "# Here we are going to use Distil-RoBERTa\n",
    "language_model_name = \"distilroberta-base\"\n",
    "\n",
    "# Training Argurments\n",
    "batch_size = 32\n",
    "\n",
    "# optim\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.001\n",
    "\n",
    "# training\n",
    "epochs = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150a298",
   "metadata": {},
   "source": [
    "In this notebook we are training **RoBERTa** as a classifier. As training material we are going to use the description of each item (i.e. the english description in the wikidata entry of each item). At the end we are indeed going to use the model to extract sentence emdeddings. This will be the input, together with the properties and statistical embeddings, to a final `MLP` that will perform the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c11053",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6914680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"../datasets/train_and_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845aa629",
   "metadata": {},
   "source": [
    "Here we are only interested in the description of the item. This will be the text that we are going to use to train the encoder. For this reason we remove the other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e152a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['item', 'name', 'type', 'category', 'subcategory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0612f98",
   "metadata": {},
   "source": [
    "Now we need to map the labels to integer values. To do so we can use the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558ecb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'cultural agnostic': 0, 'cultural representative': 1, 'cultural exclusive': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66395efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(data):\n",
    "    data['label'] = labels[data['label']]\n",
    "    return data\n",
    "\n",
    "train_dataset = dataset['train'].map(map_labels)\n",
    "val_dataset = dataset['validation'].map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3265020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Since we are saving also the hidden states eval_pred.predictions is a tuple\n",
    "# of this kind ((300, 3), (7)), where (300, 3) are the logits and the second\n",
    "# element is a tuple that has to do with the hidden states.\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    f1 = evaluate.load('f1')\n",
    "\n",
    "    logits = eval_pred.predictions[0]\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = f1.compute(predictions=predictions, references=labels, average='macro')['f1']\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4b3a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(language_model_name,\n",
    "                                                                   ignore_mismatched_sizes=True,\n",
    "                                                                   output_attentions=False,\n",
    "                                                                   output_hidden_states=True,   # ‚Üê we'll need this later to extract features embeddings\n",
    "                                                                   num_labels=3) # number of the classes\n",
    "# Load the pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"description\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6bf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_training_dataset = train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7667ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1091009",
   "metadata": {},
   "source": [
    "Let's define the training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3101a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_dir\",                    # output directory [Mandatory]\n",
    "    num_train_epochs=epochs,                      # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,       # batch size per device during training\n",
    "    warmup_steps= 20,                             # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,                    # strength of weight decay\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=learning_rate,                  # learning rate\n",
    "    report_to=\"none\",\n",
    "    logging_dir=\"cultural_classification_logs\",         # use it later to get the training curves\n",
    "    logging_steps=30\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6804bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3287/4271917279.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_training_dataset,\n",
    "   eval_dataset=tokenized_val_dataset,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a321ac",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec66faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjergj/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='196' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [196/196 10:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.638900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.611700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=196, training_loss=0.6976619253353197, metrics={'train_runtime': 649.3975, 'train_samples_per_second': 9.626, 'train_steps_per_second': 0.302, 'total_flos': 113212486975140.0, 'train_loss': 0.6976619253353197, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9a30f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/gjergj/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Sun Apr 20 14:12:12 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/gjergj/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Sun Apr 20 14:12:14 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6637540459632874,\n",
       " 'eval_accuracy': 0.68,\n",
       " 'eval_f1': 0.6586038883732849,\n",
       " 'eval_runtime': 11.1945,\n",
       " 'eval_samples_per_second': 26.799,\n",
       " 'eval_steps_per_second': 3.395,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8899b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/RoBERTa/tokenizer_config.json',\n",
       " '../models/RoBERTa/special_tokens_map.json',\n",
       " '../models/RoBERTa/vocab.json',\n",
       " '../models/RoBERTa/merges.txt',\n",
       " '../models/RoBERTa/added_tokens.json',\n",
       " '../models/RoBERTa/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"../models/RoBERTa\")\n",
    "tokenizer.save_pretrained(\"../models/RoBERTa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
